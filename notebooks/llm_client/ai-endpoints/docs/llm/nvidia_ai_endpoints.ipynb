{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6caafa",
   "metadata": {
    "id": "cc6caafa"
   },
   "source": [
    "# NVIDIA AI Foundation Endpoints\n",
    "\n",
    "The `NVIDIA` and `ImageGenNVIDIA` classes are LLM-inheriting connectors that interface with the [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/).\n",
    "\n",
    "\n",
    "> [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/) give users easy access to NVIDIA hosted API endpoints for NVIDIA AI Foundation Models like Mixtral 8x7B, Llama 2, Stable Diffusion, etc. These models, hosted on the [NVIDIA NGC catalog](https://catalog.ngc.nvidia.com/ai-foundation-models), are optimized, tested, and hosted on the NVIDIA AI platform, making them fast and easy to evaluate, further customize, and seamlessly run at peak performance on any accelerated stack.\n",
    "> \n",
    "> With [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), you can get quick results from a fully accelerated stack running on [NVIDIA DGX Cloud](https://www.nvidia.com/en-us/data-center/dgx-cloud/). Once customized, these models can be deployed anywhere with enterprise-grade security, stability, and support using [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/).\n",
    "> \n",
    "> These models can be easily accessed via the [`langchain-nvidia-ai-endpoints`](https://pypi.org/project/langchain-nvidia-ai-endpoints/) package, as shown below.\n",
    "\n",
    "This example goes over how to use LangChain to interact with and develop LLM-powered systems using the publicly-accessible AI Foundation endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be90a9",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1bd5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff689e",
   "metadata": {
    "id": "ccff689e"
   },
   "source": [
    "## Setup\n",
    "\n",
    "**To get started:**\n",
    "\n",
    "1. Create a free account with the [NVIDIA NGC](https://catalog.ngc.nvidia.com/) service, which hosts AI solution catalogs, containers, models, etc.\n",
    "\n",
    "2. Navigate to `Catalog > AI Foundation Models > (Model with API endpoint)`.\n",
    "\n",
    "3. Select the `API` option and click `Generate Key`.\n",
    "\n",
    "4. Save the generated key as `NVIDIA_API_KEY`. From there, you should have access to the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686c4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1520b31",
   "metadata": {},
   "source": [
    "## Accessing Raw LLM Endpoints\n",
    "\n",
    "Though progressively becoming less common in services, raw LLM interfaces offer the most amount of flexibility with regard to prompting and output sampling. Unlike chat models which operate on `List[ChatMessages] -> AIMessage` logic, raw endpoints expect a \"prompt\" as input and return \"text\" as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb443e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='ai-mixtral-8x22b', model_type='completion'),\n",
       " Model(id='playground_starcoder2_15b', model_type='completion')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Core LC Chat Interface\n",
    "from langchain_nvidia_ai_endpoints.llm import NVIDIA\n",
    "\n",
    "NVIDIA.get_available_models(\"nvcf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808363f",
   "metadata": {},
   "source": [
    "As we can see, the AI Foundation Endpoints currently only host a single publicly-available LLM endpoint: `starcoder2_15b`. [**`StarCoder2`**](https://github.com/bigcode-project/starcoder2) is a model developed by [BigCode](https://www.bigcode-project.org) in collaboration with NVIDIA, and you can read more about the model in the [related blog post](https://developer.nvidia.com/blog/unlock-your-llm-coding-potential-with-starcoder2/). The following will import the model for us: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Jdl2NUfMhi4J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdl2NUfMhi4J",
    "outputId": "e9c4cc72-8db6-414b-d8e9-95de93fc5db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fizzbuzz(n):\n",
      "    for i in range(1, n+1):\n",
      "        if i % 3 == 0 and i % 5 == 0:\n",
      "            print(\"FizzBuzz\")\n",
      "        elif i % 3 == 0:\n",
      "            print(\"Fizz\")\n",
      "        elif i % 5 == 0:\n",
      "            print(\"Buzz\")\n",
      "        else:\n",
      "            print(i)\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints.llm import NVIDIA\n",
    "\n",
    "starcoder = NVIDIA(model=\"starcoder2_15b\")\n",
    "\n",
    "print(starcoder.invoke(\"Here is my implementation of fizzbuzz:\\n```python\\n\", stop=\"```\"))\n",
    "# for txt in starcoder.stream(\"Here is my implementation of fizzbuzz:\\n```python\\n\", stop=[\"```\"]):\n",
    "#     print(txt, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef2181",
   "metadata": {},
   "source": [
    "Reading through the [related publication](https://arxiv.org/abs/2402.19173), we can see that it is especially suited for code completion, so in this case a message-based interface is suboptimal. Instead, it may be more useful to use the appropriate tags (i.e. `<jupyter_code>` and `<intermediate_to_code>`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6658b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors from <jupyter_code>\n",
      "# 1. Import the necessary libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# 2. Read the data as a data frame\n",
      "df = pd.read_csv(\"https://raw.githubusercontent.com/insaid20\n",
      "\n",
      "Priors from <code_to_intermediate>\n",
      "define void @_QQmain() local_unnamed_addr {\n",
      "  %1 = alloca i32, align 4\n",
      "  %2 = alloca i32, align 4\n",
      "  %3 = tail call ptr @_FortranAioBeginExternalListInput(i32 -1, ptr nonnull @_QQ\n",
      "\n",
      "Priors from <intermediate_to_code>\n",
      "#include <iostream>\n",
      "#include <string>\n",
      "#include <vector>\n",
      "#include <algorithm>\n",
      "#include <sstream>\n",
      "#include <queue>\n",
      "#include <deque>\n",
      "#include <bitset>\n",
      "#include <iterator>\n",
      "#include <list>\n",
      "#include"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints.llm import NVIDIA\n",
    "\n",
    "starcoder = NVIDIA(model=\"starcoder2_15b\", frequency_penalty=2, max_tokens=64)\n",
    "\n",
    "print(\"Priors from <jupyter_code>\")\n",
    "for txt in starcoder.stream(\"<jupyter_code>\\n\", stop=[\"</jupyter_code>\"]):\n",
    "    print(txt, end=\"\")\n",
    "\n",
    "print(\"\\n\\nPriors from <code_to_intermediate>\")\n",
    "for txt in starcoder.stream(\"<code_to_intermediate>\\n\", stop=[\"</code_to_intermediate>\"]):\n",
    "    print(txt, end=\"\")\n",
    "\n",
    "print(\"\\n\\nPriors from <intermediate_to_code>\")\n",
    "for txt in starcoder.stream(\"<intermediate_to_code>\\n\", stop=[\"</intermediate_to_code>\"]):\n",
    "    print(txt, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d37987-d568-4a73-9d2a-8bd86323f8bf",
   "metadata": {},
   "source": [
    "### Stream, Batch, and Async\n",
    "\n",
    "These models natively support streaming and expose a batch method to handle concurrent requests, as well as async methods for invoke, stream, and batch. Below are a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01fa5095-be72-47b0-8247-e9fac799435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['## 00:00:00.000000000\\n\\n* <NAME>: [MUSIC PLAYING]\\n\\n## 00:00:03.000000000\\n\\n* <NAME>:', \"What's 2*7?\\n\\nWhat's 2*8?\\n\\nWhat's 2*9?\\n\\nWhat's 2*10?\\n\\nWhat's 3*1?\\n\\nWhat's 3*2?\\n\\nWhat's 3*3?\"]\n"
     ]
    }
   ],
   "source": [
    "print(starcoder.batch([\"What's 2*3?\", \"What's 2*6?\"]))\n",
    "# Or via the async API\n",
    "# await starcoder.abatch([\"What's 2*3?\", \"What's 2*6?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75189ac6-e13f-414f-9064-075c77d6e754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A| se|ag|ull| can| fly| 2|0| km| in| one| hour|.| How| far| can| it| fly| in| one| day|?|\n",
      "â€¢| A|ircraft|\n",
      "The| plane| f|lies| at| an| alt|itude| of| 6|5|0|0| m| above| the| ground| at| speed| 7|7|7| km|/|h|.| At| what| alt|itude| will| the||"
     ]
    }
   ],
   "source": [
    "for chunk in starcoder.stream(\"How far can a seagull fly in one day?\"):\n",
    "    # Show the token separations\n",
    "    print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9a4122-7a10-40c0-a979-82a769ce7f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon|arch| butter|f|lies| migrate| from| M|ex|ico| to| California| every| year|.| They| spend| about| 1|0| months| in| M|ex|ico| and| 1|0| months| in| California|.|\n",
      "\n",
      "###| Example| Question| #|1| :| How| Long| Does| It| Take| For| Mon|arch| B|utter|f|lies| To| Migrate|?|\n",
      "\n",
      "How| long| does||"
     ]
    }
   ],
   "source": [
    "async for chunk in starcoder.astream(\n",
    "    \"How long does it take for monarch butterflies to migrate?\"\n",
    "):\n",
    "    print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a57eb5",
   "metadata": {},
   "source": [
    "## Using with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f24ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='babbage-002', model_type='completion'),\n",
       " Model(id='davinci-002', model_type='completion'),\n",
       " Model(id='gpt-3.5-turbo-instruct-0914', model_type='completion'),\n",
       " Model(id='gpt-3.5-turbo-instruct', model_type='completion')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints.llm import NVIDIA\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OPENAI Key: \")\n",
    "\n",
    "llm = NVIDIA().mode(\"openai\")\n",
    "llm.available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8382fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying Out Invoke\n",
      "[visited: 212] import random\n",
      "[visited: 213] def testme(i):\n",
      "[visited: 214]     if no_nums(i):\n",
      "[visited: 215]         return \"FizzBuzz\"\n",
      "[visited: 216]     else:\n",
      "[visited: 220]         return int(i % 3 == 0 and i % 5 == 0)\n",
      "[visited: 221] def no_nums(i):\n",
      "[visited: 222]     return True if i < 40 \\\n",
      "[visited: 247]         else return False\n",
      "[visited: 248] for i in range(1, 41):\n",
      "[visited: 249]     yield testme(i)\n",
      "[visited:                             ]unittest.main\n",
      "\n",
      "\n",
      "Trying Out Streaming\n",
      "Here is my simple implementation of fizzbuzz in python with minimal documentation:\n",
      "```\n",
      "def fizzbuzz(n):\n",
      "    # compute count packed by whole numbers\n",
      "    count = 0\n",
      "    while count < n:\n",
      "        if count % 5 == 0:\n",
      "            ans = \"fizzbuzz\"\n",
      "        if count % 3 == 0:\n",
      "            ans = \"fizz\"\n",
      "        if count % 2 == 0:\n",
      "            ans = \"buzz\"\n",
      "        count += 1\n",
      "        return printf(\"%d: %s%rs\", count, ans)\n",
      "    \n",
      "    \n",
      "    printf(\"%d:\", n)\n",
      "    return \"\"\n",
      "    \n",
      "    \n",
      "def printf(str):\n",
      "    # print a string (necessarily white space terminated)\n",
      "    print(str)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    n = int(input())\n",
      "    print(fizzbuzz(n))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = NVIDIA(model=\"davinci-002\").mode(\"openai\")\n",
    "\n",
    "print(\"Trying Out Invoke\")\n",
    "print(llm.invoke(\"Here is my implementation of fizzbuzz with minimal documentation:\\n```python\\n\", stop=\"```\", max_tokens=300))\n",
    "\n",
    "print(\"\\nTrying Out Streaming\")\n",
    "starter = \"Here is my simple implementation of fizzbuzz in python with minimal documentation:\\n```\\ndef fizzbuzz(n):\\n    #\"\n",
    "print(starter, end=\"\")\n",
    "for txt in llm.stream(starter, stop=[\"```\"], max_tokens=300):\n",
    "    print(txt, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77d11cfe-f801-44f4-9130-df0cb8f6ffc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.openai.com/v1/completions',\n",
       " 'headers': {'Accept': 'text/event-stream',\n",
       "  'content-type': 'application/json',\n",
       "  'Authorization': SecretStr('**********'),\n",
       "  'User-Agent': 'langchain-nvidia-ai-endpoints'},\n",
       " 'json': {'prompt': 'Here is my simple implementation of fizzbuzz in python with minimal documentation:\\n```\\ndef fizzbuzz(n):\\n    #',\n",
       "  'model': 'davinci-002',\n",
       "  'stream': True,\n",
       "  'stop': ['```'],\n",
       "  'max_tokens': 300},\n",
       " 'stream': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.client.last_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685ba7b-3b0c-49d4-a412-97129dccdbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
