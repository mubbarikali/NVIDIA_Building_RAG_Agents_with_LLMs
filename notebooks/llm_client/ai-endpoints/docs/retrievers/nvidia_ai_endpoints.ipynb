{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDDVue_1cq6d"
   },
   "source": [
    "# NVIDIA AI Foundation Endpoints \n",
    "\n",
    "> [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/) give users easy access to NVIDIA hosted API endpoints for NVIDIA AI Foundation Models like Mixtral 8x7B, Llama 2, Stable Diffusion, etc. These models, hosted on the [NVIDIA NGC catalog](https://catalog.ngc.nvidia.com/ai-foundation-models), are optimized, tested, and hosted on the NVIDIA AI platform, making them fast and easy to evaluate, further customize, and seamlessly run at peak performance on any accelerated stack.\n",
    "> \n",
    "> With [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), you can get quick results from a fully accelerated stack running on [NVIDIA DGX Cloud](https://www.nvidia.com/en-us/data-center/dgx-cloud/). Once customized, these models can be deployed anywhere with enterprise-grade security, stability, and support using [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/).\n",
    "> \n",
    "> These models can be easily accessed via the [`langchain-nvidia-ai-endpoints`](https://pypi.org/project/langchain-nvidia-ai-endpoints/) package, as shown below.\n",
    "\n",
    "This example goes over how to use LangChain to interact with the supported [NVIDIA Reranker Model](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nvolve-40k) for [retrieval-augmented generation](https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/) via the `NVIDIAEmbeddings` class.\n",
    "\n",
    "For more information on accessing the chat models through this api, check out the [ChatNVIDIA](../chat/nvidia_ai_endpoints) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet langchain-nvidia-ai-endpoints\n",
    "# %pip install --upgrade --quiet langchain langchain-community langchain-text-splitters\n",
    "# %pip install --upgrade --quiet faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKcxQMFTcwWi"
   },
   "source": [
    "## Setup\n",
    "\n",
    "**To get started:**\n",
    "\n",
    "1. Create a free account with the [NVIDIA NGC](https://catalog.ngc.nvidia.com/) service, which hosts AI solution catalogs, containers, models, etc.\n",
    "\n",
    "2. Navigate to `Catalog > AI Foundation Models > (Model with API endpoint)`.\n",
    "\n",
    "3. Select the `API` option and click `Generate Key`.\n",
    "\n",
    "4. Save the generated key as `NVIDIA_API_KEY`. From there, you should have access to the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoF41-tNczS3",
    "outputId": "7f2833dc-191c-4d73-b823-7b2745a93a2f"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l185et2kc8pS"
   },
   "source": [
    "## Initialization\n",
    "\n",
    "Let's list out some of the models we will be using for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='nvidia/rerank-qa-mistral-4b', model_type='ranking')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import (\n",
    "    ChatNVIDIA,\n",
    "    NVIDIAEmbeddings,\n",
    "    NVIDIARerank,\n",
    ")\n",
    "\n",
    "NVIDIARerank.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='nvidia/embed-qa-4', model_type='embedding'),\n",
       " Model(id='snowflake/arctic-embed-l', model_type='embedding')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NVIDIAEmbeddings.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat'),\n",
       " Model(id='mistralai/mistral-large', model_type='chat'),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat'),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatNVIDIA.get_available_models(filter=\"mistralai/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the list above, we should be able to see the following models:\n",
    "- `ai-mixtral-8x7b-instruct`: A NIM-containerized Mixtral-8x7b model which we will use as our LLM backbone via `ChatNVIDIA`.\n",
    "- `ai-embed-qa-4`: A NIM-containterized query-answer embedding model based on the e5-large architecture which we will use to generate embeddings via `NVIDIAEmbeddings`.\n",
    "- `ai-rerank-qa-mistral-4b`: A NIM-containerized mistral-backed question-answer reranking model which we will use to rank question-answer pairs via `NVIDIARerank`.\n",
    "\n",
    "In this notebook, we will focus on the **Reranking Model** which evaluates the relevance of passages in making decisions about a query. They are a common component of a retrieval-augmented generation pipeline and allow you to access quick relevance scores to help rank, order, filter, or otherwise process your retrieval. \n",
    "\n",
    "Let's initialize these models for use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hbXmJssPdIPX"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "\n",
    "# llm = ChatNVIDIA(model=\"ai-mixtral-8x7b-instruct\")\n",
    "# embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "# reranker = NVIDIARerank(model=\"ai-rerank-qa-mistral-4b\")\n",
    "\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\")\n",
    "reranker = NVIDIARerank(model=\"nvidia/rerank-qa-mistral-4b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc Snippets:\n",
      "'One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court o'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 73}, 'type': 'Document'}\n",
      "'As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appear'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 79}, 'type': 'Document'}\n",
      "'And I know you’re tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  \\nwe are moving fo'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 55}, 'type': 'Document'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = TextLoader(\"../../modules/state_of_the_union.txt\",).load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "for idx, text in enumerate(texts):\n",
    "    text.metadata[\"id\"] = idx\n",
    "\n",
    "retriever = FAISS.from_documents(texts, embedder).as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "print(\"\\nDoc Snippets:\")\n",
    "for doc in docs:\n",
    "    print(repr(doc.page_content[:200])+\"...\")\n",
    "    print({k:v for k,v in doc.dict().items() if k != \"page_content\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What did the president say about Ketanji Brown Jackson\n",
      "\n",
      "Most Relevant Chunks:\n",
      "'One of the most serious constitutional responsibilities a President has is nominating someone to ser'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 73, 'relevance_score': 0.1844482421875}, 'type': 'Document'}\n",
      "'As I said last year, especially to our younger transgender Americans, I will always have your back a'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 79, 'relevance_score': -16.078125}, 'type': 'Document'}\n",
      "'And I know you’re tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progres'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 55, 'relevance_score': -18.046875}, 'type': 'Document'}\n",
      "\n",
      "'Relevant' Documents:\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "top_docs = reranker.compress_documents(docs, query, top_n=5)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "\n",
    "print(\"\\nMost Relevant Chunks:\")\n",
    "for doc in top_docs:\n",
    "    print(repr(doc.page_content[:100])+\"...\")\n",
    "    print({k:v for k,v in doc.dict().items() if k != \"page_content\"})\n",
    "\n",
    "print(\"\\n'Relevant' Documents:\")\n",
    "for doc in top_docs:\n",
    "    if doc.metadata.get('relevance_score') > 0:\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rankings': [{'index': 0, 'logit': 0.1844482421875},\n",
       "  {'index': 1, 'logit': -16.078125},\n",
       "  {'index': 2, 'logit': -18.046875}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker.client.last_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Documents: [73, 79, 55]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
       " 'result': \" The president, Joe Biden, nominated Ketanji Brown Jackson to serve on the United States Supreme Court four days ago. Judge Jackson is one of the nation's top legal minds and will continue Justice Breyer's legacy of excellence.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")\n",
    "print(\"Most Relevant Documents:\", [doc.metadata[\"id\"] for doc in compressed_docs])\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
